{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "from keras import __version__\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from cnn import mini_XCEPTION\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from cnn import mini_XCEPTION\n",
    "from utils.datasets import DataManager\n",
    "from utils.datasets import split_data\n",
    "from utils.preprocessor import preprocess_input\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAT_SIZE = 32\n",
    "num_epochs = 10000\n",
    "input_shape = (64, 64, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = '/data/emotion_models/'\n",
    "IM_WIDTH, IM_HEIGHT, _ = input_shape #fixed size for InceptionV3\n",
    "NB_EPOCHS = 10000\n",
    "\n",
    "FC_SIZE = 1024\n",
    "NB_LAYERS_TO_REFINE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note for this one the data dir is SFEW\n",
    "\n",
    "dataset_name = 'SFEW'\n",
    "\n",
    "log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                              patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
    "                                                save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "train_data_dir = \"/data/SFEW_Face/Train_Aligned_Faces\"\n",
    "val_data_dir = \"/data/SFEW_Face/Val_Aligned_Faces\"\n",
    "weights_path = \"/data/emotion_models/fer2013_mini_XCEPTION.94-0.62.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 891 images belonging to 7 classes.\n",
      "Found 431 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# this time use the proper validation data\n",
    "data_generator = ImageDataGenerator(\n",
    "                        preprocessing_function=preprocess_input,\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "    \n",
    "train_generator = data_generator.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "batch_size=BAT_SIZE,\n",
    "color_mode=\"grayscale\")\n",
    "\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "val_data_dir, \n",
    "target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "batch_size=BAT_SIZE,\n",
    "color_mode=\"grayscale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 64, 1)\n",
      "(64, 64, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = train_generator.next()\n",
    "# for i in range(0,1):\n",
    "#     image = x[i]\n",
    "#     plt.imshow(image.transpose(2,1,0))\n",
    "#     plt.show()\n",
    "print(x.shape)\n",
    "image = x[2]\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(IM_WIDTH, IM_HEIGHT))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_to_finetune(model):\n",
    "    \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the top 3 layers (only one with conv2d).\n",
    "    model: _mini_XCEPTION\n",
    "    \"\"\"\n",
    "    for layer in model.layers[:-1*NB_LAYERS_TO_REFINE]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[-1*NB_LAYERS_TO_REFINE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_to_finetune(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 62, 62, 8)    72          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 62, 62, 8)    32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 62, 62, 8)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 60, 60, 8)    576         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 60, 60, 8)    32          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 60, 60, 8)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 60, 60, 16)   200         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 60, 60, 16)   64          separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 60, 60, 16)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 60, 60, 16)   400         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 60, 60, 16)   64          separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 30, 30, 16)   128         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 30, 30, 16)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 30, 30, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 30, 30, 16)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 30, 30, 32)   656         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 30, 30, 32)   128         separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 30, 30, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 30, 30, 32)   1312        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 30, 30, 32)   128         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 15, 15, 32)   512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 15, 15, 32)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 15, 15, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 15, 15, 32)   0           max_pooling2d_6[0][0]            \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 15, 15, 64)   2336        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 15, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 15, 15, 64)   4672        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 64)     2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 64)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 64)     256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 8, 8, 128)    8768        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 8, 8, 128)    17536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 128)    8192        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 128)    512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 128)    0           max_pooling2d_8[0][0]            \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 4, 4, 7)      8071        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 7)            0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 8,071\n",
      "Non-trainable params: 50,352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment below to actually run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time. time()\n",
    "\n",
    "\n",
    "# refine_model.fit_generator( \n",
    "#     train_generator,\n",
    "#     steps_per_epoch = train_generator.samples // BAT_SIZE,\n",
    "#     validation_data = validation_generator, \n",
    "#     validation_steps = validation_generator.samples // BAT_SIZE,\n",
    "#     epochs=NB_EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n",
    "# end = time. time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7177 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "### Evaluate against the FER test data set\n",
    "# this time use the proper validation data\n",
    "test_data_dir = \"/data/FER2013/PrivateTest\"\n",
    "data_generator = ImageDataGenerator(\n",
    "                        preprocessing_function=preprocess_input,\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False)\n",
    "\n",
    "    \n",
    "test_generator = data_generator.flow_from_directory(\n",
    "test_data_dir,\n",
    "target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "batch_size=1,\n",
    "color_mode=\"grayscale\",\n",
    "shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights_path = \"/data/emotion_models/SFEW_mini_XCEPTION.35-0.43.hdf5\"\n",
    "final_model = mini_XCEPTION(input_shape, num_classes)\n",
    "final_model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = test_generator.classes\n",
    "label2index = test_generator.class_indices\n",
    "idx2label = dict((v,k) for k,v in label2index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predictions = final_model.predict_generator(test_generator,steps = nb_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7177"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7177"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.where(y_classes != ground_truth)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, ..., 4, 3, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 6, 6, 6], dtype=int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5969"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 1)\n",
      "(64, 64, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = test_generator.next()\n",
    "# for i in range(0,1):\n",
    "#     image = x[i]\n",
    "#     plt.imshow(image.transpose(2,1,0))\n",
    "#     plt.show()\n",
    "print(x.shape)\n",
    "image = x[0]\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(IM_WIDTH, IM_HEIGHT))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## it seems to have worked, but for some reason the accuracy is so low, going to test again using pre-refined model\n",
    "\n",
    "## The accuracy is low because the generator automatically shuffle the data, we have to stop it otherwise it won't match the y flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "### Evaluate against the FER test data set\n",
    "# this time use the proper validation data\n",
    "test_data_dir = \"/data/SFEW_Face/Test_Aligned_Faces\"\n",
    "data_generator = ImageDataGenerator(\n",
    "                        preprocessing_function=preprocess_input,\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False)\n",
    "\n",
    "    \n",
    "test_generator = data_generator.flow_from_directory(\n",
    "test_data_dir,\n",
    "target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "batch_size=1,\n",
    "color_mode=\"grayscale\",\n",
    "shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predictions = model.predict_generator(test_generator,steps = nb_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes_old = predictions.argmax(axis=-1)\n",
    "errors = np.where(y_classes_old != ground_truth)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5922"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251358506339697\n"
     ]
    }
   ],
   "source": [
    "print(5922.0/7177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 431 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "### why accuracy so low! check again\n",
    "test_data_dir = \"/data/SFEW_Face/Val_Aligned_Faces\"\n",
    "\n",
    " \n",
    "test_generator = data_generator.flow_from_directory(\n",
    "test_data_dir,\n",
    "target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "batch_size=1,\n",
    "color_mode=\"grayscale\",\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = mini_XCEPTION(input_shape, num_classes)\n",
    "final_model.load_weights(final_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = test_generator.classes\n",
    "label2index = test_generator.class_indices\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predictions = final_model.predict_generator(test_generator,steps = nb_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0791631 , 0.02193147, 0.11939809, ..., 0.15370555, 0.10044391,\n",
       "        0.1069942 ],\n",
       "       [0.4036634 , 0.02254257, 0.297554  , ..., 0.07212307, 0.0312715 ,\n",
       "        0.07254452],\n",
       "       [0.06888822, 0.01881017, 0.30652082, ..., 0.083818  , 0.13310099,\n",
       "        0.12440033],\n",
       "       ...,\n",
       "       [0.87169695, 0.00189363, 0.08334683, ..., 0.01781679, 0.00523324,\n",
       "        0.01265741],\n",
       "       [0.12204959, 0.01454979, 0.18506977, ..., 0.01064551, 0.1789466 ,\n",
       "        0.47952715],\n",
       "       [0.87169695, 0.00189363, 0.08334683, ..., 0.01781679, 0.00523324,\n",
       "        0.01265741]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = predictions.argmax(axis=-1)\n",
    "len(y_classes)\n",
    "errors = np.where(y_classes != ground_truth)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41299303944315546\n"
     ]
    }
   ],
   "source": [
    "print(1-253.0/431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model_old = mini_XCEPTION(input_shape, num_classes)\n",
    "final_model_old.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_old = final_model_old.predict_generator(test_generator,steps = nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes_old = predictions_old.argmax(axis=-1)\n",
    "len(y_classes_old)\n",
    "errors_old = np.where(y_classes_old != ground_truth)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are not enough new data to change the prediction much, instead try adding another layer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.GlobalAveragePooling2D at 0x7f9c61fd6fd0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_new =  mini_XCEPTION(input_shape, num_classes)\n",
    "final_model_new.load_weights(weights_path)\n",
    "final_model_new.layers.pop() # Get rid of the classification layer\n",
    "final_model_new.layers.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont use this\n",
    "# final_model_aaa=  mini_XCEPTION(input_shape, num_classes)\n",
    "# final_model_aaa = Model( final_model_aaa.input, final_model_aaa.layers[-3].output) \n",
    "# final_model_aaa.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model_new =  mini_XCEPTION(input_shape, num_classes)\n",
    "# final_model_new.layers.pop()\n",
    "# final_model_new.layers.pop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually ran into this recently, and I believe that this is an issue with how Model was rewritten to be more graph-based. In addition to the layers property models also have an outputs property, so when you pop layers off your last layer is left dangling in the model's output (which is why your model still thinks its output size is 1000).\n",
    "\n",
    "A method to pop layers off and manage all the links correctly is needed, but until then I believe this would work: (my model compiles, but I haven't tried training it yet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ... Load pre-trained minixception model\n",
    "final_model_new =  mini_XCEPTION(input_shape, num_classes)\n",
    "final_model_new.load_weights(weights_path)\n",
    "final_model_new.layers.pop() # Get rid of the Activation layer\n",
    "final_model_new.layers[-1].outbound_nodes = []\n",
    "final_model_new.outputs = [final_model_new.layers[-1].output]\n",
    "output = final_model_new.get_layer('last_pool').output\n",
    "output = Dense(56, activation='relu')(output)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "new_model = Model(final_model_new.input, output)\n",
    "\n",
    "\n",
    "for layer in new_model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "for layer in new_model.layers[-2:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 62, 62, 8)    72          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 62, 62, 8)    32          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 62, 62, 8)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 60, 60, 8)    576         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 60, 60, 8)    32          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 60, 60, 8)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 60, 60, 16)   200         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 60, 60, 16)   64          separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 60, 60, 16)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 60, 60, 16)   400         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 60, 60, 16)   64          separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 30, 30, 16)   128         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 30, 30, 16)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 30, 30, 16)   64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 30, 30, 16)   0           max_pooling2d_9[0][0]            \n",
      "                                                                 batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 30, 30, 32)   656         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 30, 30, 32)   128         separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 30, 30, 32)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 30, 30, 32)   1312        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 30, 30, 32)   128         separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 15, 15, 32)   512         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 15, 15, 32)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 15, 15, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 15, 15, 32)   0           max_pooling2d_10[0][0]           \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 15, 15, 64)   2336        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 15, 15, 64)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 15, 15, 64)   4672        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 15, 15, 64)   256         separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     2048        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 64)     0           max_pooling2d_11[0][0]           \n",
      "                                                                 batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 8, 8, 128)    8768        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 128)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 8, 8, 128)    17536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 128)    8192        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 4, 4, 128)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 128)    512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 128)    0           max_pooling2d_12[0][0]           \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 4, 4, 7)      8071        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "last_pool (GlobalAveragePooling (None, 7)            0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 56)           448         last_pool[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 7)            399         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 59,270\n",
      "Trainable params: 847\n",
      "Non-trainable params: 58,423\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "27/27 [==============================] - 2s 89ms/step - loss: 2.0765 - acc: 0.1947 - val_loss: 1.8935 - val_acc: 0.2306\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.61407\n",
      "Epoch 2/10000\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 1.7406 - acc: 0.2872 - val_loss: 1.7913 - val_acc: 0.2882\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.61407\n",
      "Epoch 3/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.5960 - acc: 0.3744 - val_loss: 1.7077 - val_acc: 0.3108\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.61407\n",
      "Epoch 4/10000\n",
      "27/27 [==============================] - 2s 79ms/step - loss: 1.5894 - acc: 0.3977 - val_loss: 1.6817 - val_acc: 0.3365\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.61407\n",
      "Epoch 5/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4909 - acc: 0.4588 - val_loss: 1.7057 - val_acc: 0.3308\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.61407\n",
      "Epoch 6/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4584 - acc: 0.4785 - val_loss: 1.6608 - val_acc: 0.3885\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.61407\n",
      "Epoch 7/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4449 - acc: 0.4781 - val_loss: 1.7270 - val_acc: 0.3584\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.61407\n",
      "Epoch 8/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4321 - acc: 0.4679 - val_loss: 1.6436 - val_acc: 0.3734\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.61407\n",
      "Epoch 9/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4664 - acc: 0.4466 - val_loss: 1.7023 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.61407\n",
      "Epoch 10/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4283 - acc: 0.5078 - val_loss: 1.7159 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.61407\n",
      "Epoch 11/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.4049 - acc: 0.4781 - val_loss: 1.7038 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.61407\n",
      "Epoch 12/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4109 - acc: 0.4810 - val_loss: 1.7023 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.61407\n",
      "Epoch 13/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.4048 - acc: 0.4963 - val_loss: 1.7012 - val_acc: 0.3534\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.61407\n",
      "Epoch 14/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4072 - acc: 0.4794 - val_loss: 1.7131 - val_acc: 0.3659\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.61407\n",
      "Epoch 15/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3979 - acc: 0.4916 - val_loss: 1.6742 - val_acc: 0.3584\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.61407\n",
      "Epoch 16/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3874 - acc: 0.4868 - val_loss: 1.7003 - val_acc: 0.3810\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.61407\n",
      "Epoch 17/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3569 - acc: 0.4904 - val_loss: 1.6657 - val_acc: 0.3885\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.61407\n",
      "Epoch 18/10000\n",
      "27/27 [==============================] - 2s 79ms/step - loss: 1.4196 - acc: 0.4783 - val_loss: 1.6887 - val_acc: 0.3822\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.61407\n",
      "Epoch 19/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3832 - acc: 0.5046 - val_loss: 1.7020 - val_acc: 0.3835\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.61407\n",
      "Epoch 20/10000\n",
      "27/27 [==============================] - 2s 78ms/step - loss: 1.3810 - acc: 0.4919 - val_loss: 1.7158 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.61407\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/10000\n",
      "27/27 [==============================] - 2s 68ms/step - loss: 1.3764 - acc: 0.4926 - val_loss: 1.7906 - val_acc: 0.3584\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.61407\n",
      "Epoch 22/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3676 - acc: 0.4679 - val_loss: 1.6321 - val_acc: 0.4336\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.61407\n",
      "Epoch 23/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4050 - acc: 0.4924 - val_loss: 1.7806 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.61407\n",
      "Epoch 24/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3975 - acc: 0.4743 - val_loss: 1.6884 - val_acc: 0.3860\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.61407\n",
      "Epoch 25/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4130 - acc: 0.4752 - val_loss: 1.8159 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.61407\n",
      "Epoch 26/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3472 - acc: 0.5192 - val_loss: 1.6640 - val_acc: 0.3810\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.61407\n",
      "Epoch 27/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4148 - acc: 0.4871 - val_loss: 1.7767 - val_acc: 0.3459\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.61407\n",
      "Epoch 28/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3826 - acc: 0.4785 - val_loss: 1.6903 - val_acc: 0.3985\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.61407\n",
      "Epoch 29/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3900 - acc: 0.4860 - val_loss: 1.6852 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.61407\n",
      "Epoch 30/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3596 - acc: 0.5091 - val_loss: 1.8085 - val_acc: 0.3534\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.61407\n",
      "Epoch 31/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.4060 - acc: 0.4760 - val_loss: 1.7450 - val_acc: 0.3534\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.61407\n",
      "Epoch 32/10000\n",
      "27/27 [==============================] - 2s 78ms/step - loss: 1.3419 - acc: 0.5136 - val_loss: 1.7150 - val_acc: 0.3798\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.61407\n",
      "Epoch 33/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.4066 - acc: 0.4806 - val_loss: 1.7514 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.61407\n",
      "Epoch 34/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3124 - acc: 0.5330 - val_loss: 1.6854 - val_acc: 0.3960\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.61407\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 35/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3846 - acc: 0.5104 - val_loss: 1.6816 - val_acc: 0.4110\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.61407\n",
      "Epoch 36/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.4038 - acc: 0.4724 - val_loss: 1.7434 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.61407\n",
      "Epoch 37/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3566 - acc: 0.5093 - val_loss: 1.7446 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.61407\n",
      "Epoch 38/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3562 - acc: 0.5110 - val_loss: 1.6996 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.61407\n",
      "Epoch 39/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4413 - acc: 0.4766 - val_loss: 1.7486 - val_acc: 0.3910\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.61407\n",
      "Epoch 40/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3715 - acc: 0.4894 - val_loss: 1.7388 - val_acc: 0.3559\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.61407\n",
      "Epoch 41/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3675 - acc: 0.5012 - val_loss: 1.7367 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.61407\n",
      "Epoch 42/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3822 - acc: 0.5014 - val_loss: 1.7552 - val_acc: 0.3860\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.61407\n",
      "Epoch 43/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3984 - acc: 0.4683 - val_loss: 1.6786 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.61407\n",
      "Epoch 44/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3561 - acc: 0.5087 - val_loss: 1.6922 - val_acc: 0.3860\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.61407\n",
      "Epoch 45/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.4152 - acc: 0.4905 - val_loss: 1.7464 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.61407\n",
      "Epoch 46/10000\n",
      "27/27 [==============================] - 2s 78ms/step - loss: 1.3485 - acc: 0.5005 - val_loss: 1.7634 - val_acc: 0.3630\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.61407\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 47/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4029 - acc: 0.4831 - val_loss: 1.7042 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.61407\n",
      "Epoch 48/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3841 - acc: 0.4942 - val_loss: 1.7257 - val_acc: 0.4185\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.61407\n",
      "Epoch 49/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3841 - acc: 0.4897 - val_loss: 1.7284 - val_acc: 0.3559\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.61407\n",
      "Epoch 50/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3832 - acc: 0.4898 - val_loss: 1.7065 - val_acc: 0.3910\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.61407\n",
      "Epoch 51/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3836 - acc: 0.4845 - val_loss: 1.7590 - val_acc: 0.3835\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.61407\n",
      "Epoch 52/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.4001 - acc: 0.4999 - val_loss: 1.7915 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.61407\n",
      "Epoch 53/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3852 - acc: 0.4945 - val_loss: 1.6898 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.61407\n",
      "Epoch 54/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3848 - acc: 0.4956 - val_loss: 1.7684 - val_acc: 0.3709\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.61407\n",
      "Epoch 55/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3504 - acc: 0.5044 - val_loss: 1.6870 - val_acc: 0.3835\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.61407\n",
      "Epoch 56/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3542 - acc: 0.5003 - val_loss: 1.7207 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.61407\n",
      "Epoch 57/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3953 - acc: 0.4993 - val_loss: 1.7832 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.61407\n",
      "Epoch 58/10000\n",
      "27/27 [==============================] - 2s 74ms/step - loss: 1.3705 - acc: 0.5084 - val_loss: 1.6985 - val_acc: 0.3910\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.61407\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 59/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3743 - acc: 0.4781 - val_loss: 1.7345 - val_acc: 0.3860\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.61407\n",
      "Epoch 60/10000\n",
      "27/27 [==============================] - 2s 78ms/step - loss: 1.3790 - acc: 0.5009 - val_loss: 1.7385 - val_acc: 0.3726\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.61407\n",
      "Epoch 61/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3980 - acc: 0.5006 - val_loss: 1.7236 - val_acc: 0.3659\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.61407\n",
      "Epoch 62/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3548 - acc: 0.5105 - val_loss: 1.6856 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.61407\n",
      "Epoch 63/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3827 - acc: 0.4898 - val_loss: 1.7437 - val_acc: 0.3860\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.61407\n",
      "Epoch 64/10000\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 1.3602 - acc: 0.5059 - val_loss: 1.7487 - val_acc: 0.3659\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.61407\n",
      "Epoch 65/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3809 - acc: 0.4894 - val_loss: 1.6369 - val_acc: 0.3985\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.61407\n",
      "Epoch 66/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3718 - acc: 0.5005 - val_loss: 1.7091 - val_acc: 0.3960\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.61407\n",
      "Epoch 67/10000\n",
      "27/27 [==============================] - 2s 76ms/step - loss: 1.3589 - acc: 0.5115 - val_loss: 1.7492 - val_acc: 0.3459\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.61407\n",
      "Epoch 68/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3657 - acc: 0.4988 - val_loss: 1.7310 - val_acc: 0.3609\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.61407\n",
      "Epoch 69/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3901 - acc: 0.4929 - val_loss: 1.7241 - val_acc: 0.3935\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.61407\n",
      "Epoch 70/10000\n",
      "27/27 [==============================] - 2s 77ms/step - loss: 1.3944 - acc: 0.4877 - val_loss: 1.7006 - val_acc: 0.3684\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.61407\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 71/10000\n",
      "27/27 [==============================] - 2s 74ms/step - loss: 1.3334 - acc: 0.5202 - val_loss: 1.8007 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.61407\n",
      "Epoch 72/10000\n",
      "27/27 [==============================] - 2s 73ms/step - loss: 1.3947 - acc: 0.4860 - val_loss: 1.6663 - val_acc: 0.3885\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.61407\n",
      "149.46879768371582\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time. time()\n",
    "\n",
    "\n",
    "new_model.fit_generator( \n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // BAT_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // BAT_SIZE,\n",
    "    epochs=NB_EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n",
    "end = time. time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 00179: val_loss did not improve from 1.61407\n",
    "Epoch 180/10000\n",
    "27/27 [==============================] - 2s 76ms/step - loss: 1.3928 - acc: 0.4879 - val_loss: 1.7044 - val_acc: 0.3985\n",
    "\n",
    "Epoch 00180: val_loss did not improve from 1.61407\n",
    "374.157114982605\n",
    "\n",
    "\n",
    "Epoch 00071: val_loss did not improve from 1.61407\n",
    "Epoch 72/10000\n",
    "27/27 [==============================] - 2s 73ms/step - loss: 1.3947 - acc: 0.4860 - val_loss: 1.6663 - val_acc: 0.3885\n",
    "\n",
    "Epoch 00072: val_loss did not improve from 1.61407\n",
    "149.46879768371582"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
